{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, random, sbmtm\n",
    "import seaborn as sns; sns.set(style=\"white\", color_codes=True) # is not installed on system, just folder in this directory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sbmtm import sbmtm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_books = [] # in case something breaks, we can pick up where we were\n",
    "\n",
    "# calculates bin counts for binning documents through time\n",
    "def calc_bin_sizes(count_docs, num_bins):\n",
    "    min_split_count = int(count_docs/num_bins)\n",
    "    extras = count_docs- min_split_count*num_bins\n",
    "\n",
    "    counts = [min_split_count for i in range(num_bins)]\n",
    "    for indx in random.sample(range(num_bins),extras):\n",
    "        counts[indx] += 1\n",
    "        \n",
    "    return counts\n",
    "\n",
    "def calc_topic_fractions(model, data_frame, bin_counts, weight_type=\"summed\"):\n",
    "    \n",
    "    binned_data = []\n",
    "    current_place = 0\n",
    "    \n",
    "    whole_index = list(data_frame.index)\n",
    "    count_topics = len(model.topics())\n",
    "    \n",
    "    for bin_cnt in bin_counts:\n",
    "        \n",
    "        series = [0 for i in range(count_topics)]\n",
    "        for doc in whole_index[current_place:current_place+bin_cnt]:\n",
    "            for topic, weight in model.topicdist(doc):\n",
    "                if weight_type == \"binary\":\n",
    "                    if weight != 0:\n",
    "                        series[topic] += 1\n",
    "                elif weight_type == \"summed\":\n",
    "                    series[topic] += weight\n",
    "                else: #weighted (by word count)\n",
    "                    num_words = data_frame[\"word_count\"][doc]\n",
    "                    series[topic] += weight*num_words\n",
    "                    \n",
    "        total = sum(series)\n",
    "        normed = [i/total for i in series]\n",
    "        binned_data.append(normed)\n",
    "        \n",
    "        # seaborn wants the series values to be in separate lists, unlike pyplot\n",
    "        sns_flip = [[] for i in range(count_topics)]\n",
    "        for series in binned_data:\n",
    "            for indx, value in enumerate(series):\n",
    "                sns_flip[indx].append(value)\n",
    "    \n",
    "    return sns_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_posts 17934521-shotgun-lovesongs.json\n",
      "creating model for shotgun lovesongs\n",
      "cleaned_posts 17225311-tampa.json\n",
      "creating model for tampa\n"
     ]
    }
   ],
   "source": [
    "threshold = 25  # the threshold number of words in a review for it to be included. \n",
    "                # Any with less than 10 has already been excluded.\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "for directory, sub, files in os.walk(\"cleaned_posts\"):\n",
    "    \n",
    "    # We'll use the parent directory to output files later\n",
    "    parent = directory.replace(\"cleaned_posts\",\"\")\n",
    "    \n",
    "    for file in files: #iterating over the book files\n",
    "        if file not in finished_books and file != \".DS_Store\":\n",
    "            print(directory, file)\n",
    "            with open(directory+\"/\"+file,\"r\",encoding=\"utf8\") as f:\n",
    "                \n",
    "                title = \" \".join(file.replace(\".json\",\"\").split(\"-\")[1:]) # the title of the book\n",
    "                \n",
    "                df = pd.DataFrame(pd.read_json(f))\n",
    "                df = df.transpose() #I guess I saved the dataframe in the wrong orientation, so we flip it here\n",
    "                \n",
    "                df.sort_values([\"delta\"],inplace=True) # sort from earliest to latest\n",
    "\n",
    "                # now we look at subset of the data. There is also something strange with what \n",
    "                # Seaborn wants as an index for the data and this sort helps fix that\n",
    "                temp_sub_frame = df.loc[df[\"word_count\"]>threshold]\n",
    "                \n",
    "                # data selection in Pandas returns a Series and for some reason Seaborn wants a list here\n",
    "                sub_frame = pd.DataFrame({\"delta\":list(temp_sub_frame[\"delta\"]),\n",
    "                                          \"word_count\":list(temp_sub_frame[\"word_count\"]),\n",
    "                                          \"text\":list(temp_sub_frame[\"text\"]),\n",
    "                                          \"name\":list(temp_sub_frame[\"name\"])})\n",
    "                \n",
    "                count_reviews = len(sub_frame)\n",
    "                \n",
    "                if not os.path.isfile(parent+\"descriptive_plots/{}_{}_joint.png\".format(title,threshold)):\n",
    "                    # plotting the data\n",
    "                    min_x = min(sub_frame[\"delta\"])*1.25\n",
    "                    max_x = max(sub_frame[\"delta\"])*1.25\n",
    "                    g = sns.jointplot(x=\"delta\",\n",
    "                                      y=\"word_count\",\n",
    "                                      data=sub_frame,\n",
    "                                      xlim=[min_x,max_x],\n",
    "                                      kind=\"reg\")\n",
    "                    g = g.plot(sns.regplot, sns.distplot)\n",
    "                    g.fig.suptitle(title)\n",
    "                    max_y = max(sub_frame[\"word_count\"])\n",
    "                    g.fig.text(.65,.65, \"{} reviews\".format(count_reviews))\n",
    "                    \n",
    "                    g.savefig(\"descriptive_plots/{}_{}_joint.png\".format(title,threshold),dpi=300) \n",
    "                    g.fig.clear()\n",
    "                \n",
    "                # Now we run topSBM\n",
    "                \n",
    "                model = sbmtm()\n",
    "                print(\"creating model for {}\".format(title))\n",
    "                model.make_graph(list(sub_frame[\"text\"]),list(sub_frame.index))\n",
    "                model.fit()\n",
    "                \n",
    "                \"\"\"\n",
    "                We save the model object because we've done the inference step now.\n",
    "                Because the stochastic realizations of the inference aren't the same, I want to save \n",
    "                any multiple states. The following while loop figures out the count of instances so \n",
    "                that we can save this instance under the right, new name.\n",
    "                \"\"\"\n",
    "                instance = 1\n",
    "                while os.path.isfile(\"model_inference_states/{}-{}-{}.pkl\".format(title,threshold,instance)):\n",
    "                      instance +=1\n",
    "                \n",
    "                with open(\"model_inference_states/{}-{}-{}.pkl\".format(title, threshold, instance),\"wb\") as outfile:\n",
    "                      outfile.write(pickle.dumps(model))\n",
    "                      \n",
    "                \n",
    "                # here is the standard plot (documents are plotted based on their group membership)\n",
    "                pos,t,tpos = model.state.draw(layout=\"bipartite\",\n",
    "                                              output_size=(1000, 1000),\n",
    "                                              output=\"graphs/{}_{}_{}_{}.png\".format(title,count_reviews, threshold, instance),\n",
    "                                              hshortcuts=1,\n",
    "                                              hide=0)\n",
    "                \n",
    "                \"\"\"\n",
    "                Now we're going to take the position arguments from that plot and replot the documents based \n",
    "                on when they appeared, not on community membership. The documents are already properly ordered\n",
    "                in the dataframe, so we just need to iterate over the indices and add the proper increment to\n",
    "                the y location.\n",
    "                \"\"\"\n",
    "                arr = pos.get_2d_array(range(5)) # 2d array (x,y) of node positions\n",
    "\n",
    "                increment = 1/len(model.documents) # the spacing increment\n",
    "                y_val = .5 # the plot y-range is [-.5,.5] and we start at the top\n",
    "\n",
    "                for i in range(len(model.documents)):\n",
    "                      arr[1][i] = y_val\n",
    "                      y_val -= increment\n",
    "                      \n",
    "                pos.set_2d_array(arr)\n",
    "\n",
    "                model.state.draw(layout=pos,\n",
    "                                 output_size=(1000, 1000),\n",
    "                                 output=\"graphs/{}_{}_{}_{}_time.png\".format(title, count_reviews, threshold,instance),\n",
    "                                 hshortcuts=1,\n",
    "                                 hide=0)\n",
    "                      \n",
    "                \"\"\"\n",
    "                Now we want to look at the change in the relative frequency of topics over time.\n",
    "                We bin the documents an then try three ways of calculating frequency; binary (does the document\n",
    "                have any weight on the topic? [y/n], sum of weightings across documents, sum of weighting across\n",
    "                documents weighted by the document length.\n",
    "                \"\"\" \n",
    "                \n",
    "                num_bins = 10 #2\n",
    "                count_docs = len(sub_frame)\n",
    "                \n",
    "                # to the best of my knowledge there is no np.linspace() with discrete spacing so a custom fnct.\n",
    "                bin_counts = calc_bin_sizes(count_docs, num_bins)\n",
    "                \n",
    "                # calculate the relative frequencies with type of weighting equal to binary, summed, or weighted\n",
    "                \n",
    "                series = calc_topic_fractions(model, sub_frame, bin_counts, weight_type=\"binary\")\n",
    "                for line in series:\n",
    "                    g = sns.lineplot(x=range(1,num_bins+1),y=line)\n",
    "                fig = g.get_figure()\n",
    "                fig.suptitle(\"{} - binary - {} reviews\".format(title,count_reviews))\n",
    "                fig.savefig(\"frequency_plots/{}_{}_{}_binary.png\".format(title, threshold, instance))\n",
    "                fig.clear()\n",
    "                \n",
    "                \n",
    "                series = calc_topic_fractions(model, sub_frame, bin_counts, weight_type=\"summed\")\n",
    "                for line in series:\n",
    "                    g = sns.lineplot(x=range(1,num_bins+1),y=line)\n",
    "                fig = g.get_figure()\n",
    "                fig.suptitle(\"{} - summed - {} reviews\".format(title,count_reviews))\n",
    "                fig.savefig(\"frequency_plots/{}_{}_{}_summed.png\".format(title, threshold, instance))\n",
    "                fig.clear()\n",
    "                \n",
    "                \n",
    "                series = calc_topic_fractions(model, sub_frame, bin_counts, weight_type=\"weighted\")\n",
    "                for line in series:\n",
    "                    g = sns.lineplot(x=range(1,num_bins+1),y=line)\n",
    "                fig = g.get_figure()\n",
    "                fig.suptitle(\"{} - weighted - {} reviews\".format(title,count_reviews))\n",
    "                fig.savefig(\"frequency_plots/{}_{}_{}_weighted.png\".format(title, threshold, instance))\n",
    "\n",
    "                finished_books.append(file)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
